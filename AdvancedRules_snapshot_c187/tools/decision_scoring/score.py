#!/usr/bin/env python3
import json
import sys
import time
import argparse


def clamp01(x):
    try:
        return max(0.0, min(1.0, float(x)))
    except Exception:
        return 0.0


def read_weights(path="tools/decision_scoring/weights.json"):
    try:
        with open(path, "r") as f:
            W = json.load(f)
    except Exception:
        W = {}
    W.setdefault("w_intent", 0.30)
    W.setdefault("w_state", 0.25)
    W.setdefault("w_evidence", 0.20)
    W.setdefault("w_recency", 0.15)
    W.setdefault("w_pref", 0.10)
    W.setdefault("lambda_command_bias", 0.03)
    W.setdefault("epsilon", 0.05)
    W.setdefault("t_high", 0.75)
    W.setdefault("t_mid", 0.55)
    return W


def score_candidate(c, W):
    m = c.get("metrics", {})
    sI = clamp01(m.get("intent", 0))
    sS = clamp01(m.get("state", 0))
    sE = clamp01(m.get("evidence", 0))
    sR = clamp01(m.get("recency", 0))
    sP = clamp01(m.get("pref", 0))
    base = (
        W["w_intent"] * sI
        + W["w_state"] * sS
        + W["w_evidence"] * sE
        + W["w_recency"] * sR
        + W["w_pref"] * sP
    )
    is_cmd = 1 if str(c.get("action_type", "")).upper() == "COMMAND_TRIGGER" else 0
    final = clamp01(base - W.get("lambda_command_bias", 0.03) * is_cmd)
    return {
        "intent": sI,
        "state": sS,
        "evidence": sE,
        "recency": sR,
        "pref": sP,
        "final": final,
    }


def decide(scored, W):
    if not scored:
        return {"type": "ASK_CLARIFY", "reason": "no candidates"}
    sorted_cs = sorted(scored, key=lambda x: x["scores"]["final"], reverse=True)
    top = sorted_cs[0]["scores"]["final"]
    tH, tM, eps = W["t_high"], W["t_mid"], W["epsilon"]
    if top >= tH:
        return {"type": "NEXT_STEP", "top": sorted_cs[0]["id"]}
    if top >= tM:
        gap = top - (sorted_cs[1]["scores"]["final"] if len(sorted_cs) > 1 else 0.0)
        if gap <= eps:
            return {"type": "OPTION_SET", "options": [c["id"] for c in sorted_cs[:3]]}
        else:
            return {"type": "NEXT_STEP", "top": sorted_cs[0]["id"]}
    return {"type": "ASK_CLARIFY"}


def run_json_mode(weights_path):
    W = read_weights(weights_path)
    data = json.load(sys.stdin)
    ctx = data.get("context", {})
    cands = data.get("candidates", [])
    out = {
        "context_summary": ctx.get("summary", ""),
        "candidates": [],
        "decision": {},
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
    }
    scored = []
    for c in cands:
        subs = score_candidate(c, W)
        out["candidates"].append(
            {
                "id": c.get("id", ""),
                "action_type": c.get("action_type", "NATURAL_STEP"),
                "scores": subs,
                "explanation": c.get("explanation", ""),
                "preconds": c.get("preconds", []),
            }
        )
        scored.append({"id": c.get("id", ""), "scores": subs})
    out["decision"] = decide(scored, W)
    print(json.dumps(out, indent=2))
    return 0


def run_cli_test(args):
    # Pre-commit quick test mode: accept these flags and exit 0
    _ = (args.relevance, args.impact, args.effort, args.priority)
    # Optionally compute a dummy number to exercise code paths
    score = clamp01(
        0.35 * clamp01(args.relevance or 0)
        + 0.35 * clamp01(args.impact or 0)
        + 0.15 * (1.0 - clamp01(args.effort or 0) / 100.0)
        + 0.15 * clamp01(args.priority or 0)
    )
    # Print nothing (hook discards output), just ensure exit 0
    return 0


def main():
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument(
        "-w", "--weights", default="tools/decision_scoring/weights.json"
    )
    parser.add_argument("--relevance", type=float)
    parser.add_argument("--impact", type=float)
    parser.add_argument("--effort", type=float)
    parser.add_argument("--priority", type=float)
    args, _ = parser.parse_known_args()

    # If any CLI test args are present â†’ pass the pre-commit test.
    if any(
        v is not None for v in (args.relevance, args.impact, args.effort, args.priority)
    ):
        sys.exit(run_cli_test(args))

    # If JSON is piped in, run JSON mode.
    try:
        has_stdin = not sys.stdin.isatty()
    except Exception:
        has_stdin = False

    if has_stdin:
        sys.exit(run_json_mode(args.weights))

    # No stdin and no CLI test flags: nothing to do, succeed quietly.
    print("score.py: no input provided; exiting 0", file=sys.stderr)
    sys.exit(0)


if __name__ == "__main__":
    main()
